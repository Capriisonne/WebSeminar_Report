\section{Visualize text corpus}

\subsection{Literature review}

The visualization of text and whole text corpus was already discussed in the papers  \cite{visualiuationCorpus1},  \cite{visualiuationCorpus2} and \cite{visualiuationCorpus3}. \\
\\
In  \cite{visualiuationCorpus1} Fortuna et al. visualized a text corpus using a two-dimensional point map. Documents where represented as points on this map. They also calculate a density for each point which was visualized with a background texture. Around each document point they draw the most common keywords for this document to give the user an idea about the content of a document. \\
To visualize the document the authors reduced the dimensionality of the document by applying linear subspace methods, like Principal Component Analysis (PCA) or Latent Semantic Indexing (LSI), and multidimensional scaling methods. \\
Linear subspace methods focus on finding direction in the original vector space. Projecting the data (text documents) only on the first two directions gives a
two-dimensional point representation of the documents. \\
The multidimensional scaling methods the document points are positioned into two dimension in a way that they minimize a given energy function. \\
The basic algorithm of this work was the following: 
\textbf{Input:} Corpus of documents to visualize in form of TDFIDF vectors \\
\textbf{Output: } Set of two dimensional points representing the documents \\
\textbf{Procedure: } \\
\begin{enumerate}
	\item Calculate k dimensional semantic space generated by input corpus of documents
	\item Project documents into the semantic space
	\item Apply multidimensional scaling using energy function on documents with Euclidean distance in semantic space as similarity measure
\end{enumerate} 
After they mapped the documents to two-dimensional points they used other techniques to make the structure of documents more explicit to a user. One technique was to generate a landscape by using the density of the points and visualizing it with a  background texture. A second technique described was to assign a set of keywords to each point, which a visualized around the document points within a circle R. \\
~\\


While \cite{visualiuationCorpus1} visualized a text corpus as a point map in two-dimensional space,  the authors of \cite{visualiuationCorpus2} used a graphic approach to visualize a text corpus. In their work they propose different method to project a term-document matrix into low-dimensional space for visualizing. Before the visualize the corpus they analyse it with a method called \textit{Centering Resonance Analysis} (CRA), which produces a stand-alone, abstract representation of the texts. CRA creates for a text T a graph G(T)=(V(T), E(T)). This graph is build as follows:
\begin{enumerate}
	\item Extract noun phrases from each sentence. A noun phrase contains nouns and adjectives. 
	\item Form the set of vertices V(T) with the words of the extracted noun phrases, such that each vertex corresponds to one word. 
	\item Establish an edge between two vertices, if the corresponding word co-occur in the same noun phrase.
\end{enumerate}

The goals for their visualization was than, that documents with similar content are close to each other, important words appear bigger and display words close to the document they are contained in. 
They propose different methods and measures to solve their problems. \\
~\\
The authors of \cite{visualiuationCorpus3}  also propose a graph structure for visualization. In comparison to \cite{visualiuationCorpus2} they just work with one document, rather than a whole corpus. \\
After they preprocessed there text (stemming, removing of stopwords), they scan the text with a 2-word gap. For each word that appears for the first time in the text a new node is generated. If a new pair appear in a 2-word gap a new edge is established between the corresponding nodes and the weight is set to 1. The weight of an already existing edge is increased by 1 if the two words appear together again. The more frequent the combination of two words, the higher is the weight of their connection. In a second pass they use a 5-word gap to identify word pairs. \\
For visualization they use a Force Atlas algorithm, which pushed cluster of nodes away from each other, while aligning the nodes that are connected to the cluster around them. In our case a cluster is a set of nodes which are highly connected. \\
The size of the nodes is set accordingly to their betweenness centrality, which indicates how often the node appears on the shortest path between any two random nodes in the network. The higher the betweeness centrality of a node, the more influential is the node in the text. \\
In a next step they detect clusters in the text in order to visualize them with different colors. \\
As a last step the identify the pathways for meaning circulation by exploring the relations betweeen the clusters and the role of the influential nodes.

\subsection{Graph based visualization} \label{sec:graphVisualization}

For our work we also decided for a graph based visualization, which is based on the ides of \cite{visualiuationCorpus3}. \\
Our goal was to visualize a summary of posts, which were written at a specific time - this could be a month or a time range like the last 14 days. \\
The algorithm we used to  create the graph G=(V,E) was than as follows: \\

\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\Input{ range of time, number k of most used word to consider }
	\Output{ Graph G = (V, E) }
	set C =\{all post which publishing date lays in the range of time\} \\
	Remove stop words from C \\
	Scan the text and count how often each word appears \\
	Order the list of distinct words by their occurrence in descending order \\
	Take the top k words from this list and form a list $  L =Â \{ k most used words\}$ \\
	\ForEach{ word w in L}
	{
		create node X with label w \\
		set the size of X accordingly to the occurrence of the word in the corpus  \\
	}
	Scan the corpus again and \\
	\ForEach{ word $w_1$ and $w_2$ in L}
	{
		\If{$w_1$ and $w_2$ co-occur together and edge E($w_1$ ,$w_2$) does not exist}
		{	
			draw edge E($w_1$ ,$w_2$)\\
			set weight of E to 1 \\
			
		}
		{
			increase weight of E by 1
		}
		
	}
	
	\caption{Algorithm to create graph for summarizing  posts}
\end{algorithm}

In our case we just used the most common English words as stop words and did not calculate new once. \\
~\\

This procedure gives us a graph, where each node has a specific size and each edge has a specific weight. This values a later used to visualize more frequent words bigger and more frequent connections thicker. 






